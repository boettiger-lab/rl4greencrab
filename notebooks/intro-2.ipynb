{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a20c1cfd-cdbc-4286-997d-d740c609b514",
   "metadata": {},
   "source": [
    "# RL training on the env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1862e7b1-b0ba-44e9-ac4d-767ee4780a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from plotnine import (\n",
    "    ggplot, aes, geom_density, geom_line, geom_point, \n",
    "    geom_violin, facet_grid, labs, theme, facet_wrap,\n",
    ")\n",
    "\n",
    "# for rl training\n",
    "from stable_baselines3 import PPO, TD3\n",
    "from sb3_contrib import TQC\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# the rl environment\n",
    "from rl4greencrab import greenCrabSimplifiedEnv as gcse\n",
    "\n",
    "# helper that paralelizes episode simulations for evaluation purposes (agent -> reward)\n",
    "from rl4greencrab import evaluate_agent\n",
    "\n",
    "# helper that creates a single episode simulation keeping track of many variables\n",
    "# of the internal env state\n",
    "from rl4greencrab import simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f093331-3c48-441a-8803-575878c35f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using 'vectorized environments' helps paralelize RL training\n",
    "# (the RL agent collects data by simultaneously interacting with\n",
    "# n_envs different environments, rather than doing it one envir.\n",
    "# at a time.\n",
    "vec_env = make_vec_env(gcse, n_envs=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8687dbfd-3478-4d46-b8be-ad9a51d65394",
   "metadata": {},
   "source": [
    "## Algo 1: PPO\n",
    "\n",
    "see docs here: https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1e854f5-5ac2-4d60-b2ed-47663a6ed86f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55300fce75b1407787e8286c9d6b662c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = PPO(\"MlpPolicy\", vec_env, verbose=0, tensorboard_log=\"/home/rstudio/logs\")\n",
    "model.learn(\n",
    "\ttotal_timesteps=250_000, \n",
    "\tprogress_bar=True,\n",
    ")\n",
    "model.save(\"ppo_gcse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cc330f-7cfb-4fa7-94f0-9ad91e680d6d",
   "metadata": {},
   "source": [
    "## Algo 2: TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c91e15-d69f-445b-8468-eb7e2adee387",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = TD3(\"MlpPolicy\", gcse, verbose=0, tensorboard_log=\"/home/rstudio/logs\")\n",
    "model.learn(\n",
    "\ttotal_timesteps=250_000, \n",
    "\tprogress_bar=True,\n",
    ")\n",
    "model.save(\"td3_gcse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4af2e35-266a-499b-b953-be61fc492a63",
   "metadata": {},
   "source": [
    "## Algo 3: TQC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5449d3e9-610a-4bfd-9d3b-554117e04eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c069130cc5ad4ef29d3f99677004484a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = TQC(\"MlpPolicy\", vec_env, verbose=0, tensorboard_log=\"/home/rstudio/logs\")\n",
    "model.learn(\n",
    "\ttotal_timesteps=250_000, \n",
    "\tprogress_bar=True,\n",
    ")\n",
    "model.save(\"tqc_gcse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b00055-d019-41f9-93f2-9514f11e5a1a",
   "metadata": {},
   "source": [
    "## Loading and evaluating trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fce88ff-4c74-4eb6-ae01-475955ab0584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load agents into the CPU (rather than the GPU - the default)\n",
    "# since the paralelization we use to evaluate agents works with\n",
    "# CPU\n",
    "\n",
    "ppoAgent = PPO.load(\"ppo_gcse\", device=\"cpu\")\n",
    "# td3Agent = TD3.load(\"td3_gcse\", device=\"cpu\")\n",
    "tqcAgent = TQC.load(\"tqc_gcse\", device=\"cpu\")\n",
    "evalEnv = gcse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43994123-2295-4aa3-a0c2-e2ed42cf7c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPS = 30\n",
    "\n",
    "ppo_rew = evaluate_agent(agent=ppoAgent, env=evalEnv, ray_remote=True).evaluate(n_eval_episodes=N_EPS)\n",
    "# td3_rew = evaluate_agent(agent=td3Agent, ray_remote=True).evaluate(n_eval_episodes=N_EPS)\n",
    "tqc_rew = evaluate_agent(agent=tqcAgent, env=evalEnv, ray_remote=True).evaluate(n_eval_episodes=N_EPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b2be83f-cf60-45e8-b5d5-764a4337b5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbbe4ab0-7131-4c52-b180-dbe43f00896a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PPO mean rew = -9.576186668916746\n",
      "TQC mean rew = -6.698807471424942\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "PPO mean rew = {ppo_rew}\n",
    "TQC mean rew = {tqc_rew}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b603baa-0bf2-4ed9-a901-86ce3a90660c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
